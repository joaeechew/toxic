{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 95851 rows, 7 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/train.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_train = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 226998 rows, 1 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/test.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_test = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with string \"unknown\"\n",
    "df_train.fillna('unknown',inplace=True)\n",
    "df_test.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/joaeechew/dev/nltk_data\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from os import listdir\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start with simpler models (more cleaning).\n",
    "\n",
    "I want to:\n",
    "- Keep exclamation marks only as it may be an indicator for toxicity\n",
    "- Keep uppercase as it might be toxic\n",
    "- Remove stopwords (to try with and without)\n",
    "- Stemming (to try with and without)\n",
    "- Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(corpus, vocab):\n",
    "    \"\"\"Takes a corpus in list format and applies basic preprocessing steps of word tokenization,\n",
    "     removing of english stop words, and lemmatization. Returns processed corpus and vocab.\"\"\"\n",
    "    processed_corpus = []\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|!]+')\n",
    "    for row in corpus:\n",
    "        word_tokens = tokenizer.tokenize(row)\n",
    "        word_tokens_no_digits = [t for t in word_tokens if not t.isdigit()]\n",
    "        word_tokens_english = [t for t in word_tokens_no_digits if t in english_words or not t.isalpha()]\n",
    "        word_tokens_no_stops = [t for t in word_tokens_english if not t in english_stopwords]\n",
    "        word_tokens_no_stops_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in word_tokens_no_stops]\n",
    "        processed_corpus.append(word_tokens_no_stops_lemmatized)\n",
    "        vocab.update(word_tokens_no_stops_lemmatized)\n",
    "    return processed_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 621 ms, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = Counter()\n",
    "df_train.comment_text, vocab = process_text(df_train.comment_text, vocab)\n",
    "df_test.comment_text, vocab = process_text(df_test.comment_text, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 379991), ('WIKI_LINK', 313510), ('article', 124798), ('page', 118990), ('The', 101726), ('use', 70128), ('would', 63711), ('one', 59963), ('edit', 58939), ('like', 55635), ('talk', 51022), ('please', 49581), ('may', 45440), ('deletion', 44252), ('see', 41889), ('image', 41563), ('think', 39360), ('link', 38259), ('also', 35300), ('make', 34680), ('time', 34584), ('know', 33068), ('information', 30459), ('people', 29877), ('used', 29380), ('need', 28992), ('copyright', 28065), ('hi', 27704), ('free', 27098), ('EXTERNAL_LINK', 26877), ('made', 26835), ('policy', 25284), ('name', 23842), ('A', 23456), ('speedy', 22820), ('way', 22464), ('could', 22357), ('source', 22282), ('add', 21650), ('content', 21491), ('want', 20840), ('even', 20830), ('section', 20686), ('vandalism', 20336), ('good', 20328), ('get', 20327), ('fair', 20209), ('work', 19822), ('help', 19776), ('well', 19106)]\n",
      "99177\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(50))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22256635</th>\n",
       "      <td>[kiss, geek, I, said, true, I, account]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450690</th>\n",
       "      <td>[vandalize, edit, W, S, continue, blocked]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54037174</th>\n",
       "      <td>[interest, I, removed, interest, section, adde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77493077</th>\n",
       "      <td>[nationality, aware, shown, support, towards, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79357270</th>\n",
       "      <td>[The, reader, going, say, ethereal, vocal, sty...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82428052</th>\n",
       "      <td>[sum, fried]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87311443</th>\n",
       "      <td>[put, English, example, people, like]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114749757</th>\n",
       "      <td>[Guy, resident, go, carnival, every, year, tow...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138560519</th>\n",
       "      <td>[far, go, article, embarrassing, fish, golden,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139353149</th>\n",
       "      <td>[hear, corrected]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment_text  toxic  \\\n",
       "id                                                                    \n",
       "22256635             [kiss, geek, I, said, true, I, account]      1   \n",
       "27450690          [vandalize, edit, W, S, continue, blocked]      0   \n",
       "54037174   [interest, I, removed, interest, section, adde...      0   \n",
       "77493077   [nationality, aware, shown, support, towards, ...      0   \n",
       "79357270   [The, reader, going, say, ethereal, vocal, sty...      0   \n",
       "82428052                                        [sum, fried]      0   \n",
       "87311443               [put, English, example, people, like]      0   \n",
       "114749757  [Guy, resident, go, carnival, every, year, tow...      0   \n",
       "138560519  [far, go, article, embarrassing, fish, golden,...      0   \n",
       "139353149                                  [hear, corrected]      0   \n",
       "\n",
       "           severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                               \n",
       "22256635              0        0       0       0              0  \n",
       "27450690              0        0       0       0              0  \n",
       "54037174              0        0       0       0              0  \n",
       "77493077              0        0       0       0              0  \n",
       "79357270              0        0       0       0              0  \n",
       "82428052              0        0       0       0              0  \n",
       "87311443              0        0       0       0              0  \n",
       "114749757             0        0       0       0              0  \n",
       "138560519             0        0       0       0              0  \n",
       "139353149             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26196\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text data = '\\n'.join(lines)\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to a vocabulary file\n",
    "\n",
    "path = 'data/processed/vocab.txt'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "save_list(tokens, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/train.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_train.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/test.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_test.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
