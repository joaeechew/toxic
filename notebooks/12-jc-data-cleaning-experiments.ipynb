{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a default CNN single model to text various cleaning steps and impact on score.\n",
    "\n",
    "Controls:\n",
    "- CNN single model\n",
    "- maxlen: 65\n",
    "- min occurance vocab: 5\n",
    "- glove.6B.100D\n",
    "- epochs: 2\n",
    "- cv: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 95851 rows, 7 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/train.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_train = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 226998 rows, 1 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/test.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_test = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/joaeechew/dev/nltk_data\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from os import listdir\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(corpus, vocab, regex=r'[\\w]+', digits=False, english_only=False, stop=False, lemmatize=False):\n",
    "    \"\"\"Takes a corpus in list format and applies basic preprocessing steps of word tokenization,\n",
    "     removing of english stop words, and lemmatization. Returns processed corpus and vocab.\"\"\"\n",
    "    processed_corpus = []\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(regex)\n",
    "    for row in corpus:\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        if digits:\n",
    "            tokens = [t for t in tokens if not t.isdigit()]\n",
    "        if english_only:\n",
    "            tokens = [t for t in tokens if t in english_words]\n",
    "        if stopwords:\n",
    "            tokens = [t for t in tokens if not t in english_stopwords]\n",
    "        if lemmatize:\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "        vocab.update(tokens)\n",
    "        tokens = ' '.join(tokens)\n",
    "        if tokens == '':\n",
    "            tokens = 'cleaned'\n",
    "        processed_corpus.append(tokens)\n",
    "    return processed_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with string \"unknown\"\n",
    "df_train.fillna('unknown',inplace=True)\n",
    "df_test.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'[\\w|!]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 251 ms, total: 19.5 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = Counter()\n",
    "df_train.comment_text, vocab = process_text(df_train.comment_text, vocab,\n",
    "                                            digits=False, english_only=False, stop=False, lemmatize=False)\n",
    "df_test.comment_text, vocab = process_text(df_test.comment_text, vocab,\n",
    "                                          digits=False, english_only=False, stop=False, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 380100), ('WIKI_LINK', 313510), ('Wikipedia', 241555), ('article', 125110), ('page', 119359), ('The', 101785), ('If', 85013), ('use', 70146), ('would', 63716), ('one', 60096), ('edit', 58999), ('like', 55648), ('talk', 51321), ('please', 49671), ('You', 47928), ('Please', 46481), ('may', 45442), ('deletion', 44264), ('WP', 43485), ('It', 42983), ('see', 41919), ('image', 41726), ('articles', 40624), ('think', 39389), ('This', 38673), ('also', 35316), ('make', 34685), ('know', 33175), ('information', 30491), ('time', 29978), ('people', 29955), ('used', 29384), ('edits', 28860), ('Image', 28460), ('copyright', 28066), ('pages', 27930), ('hi', 27761), ('deleted', 27634), ('Thanks', 27356), ('free', 27112), ('EXTERNAL_LINK', 26877), ('made', 26844), ('Thank', 26246), ('editing', 26227), ('policy', 25299), ('name', 23915), ('A', 23480), ('sources', 23425), ('questions', 23405), ('speedy', 22822), ('could', 22358), ('source', 22321), ('need', 21838), ('add', 21657), ('In', 21367), ('want', 20869), ('way', 20854), ('even', 20831), ('section', 20717), ('link', 20716), ('reverted', 20391), ('vandalism', 20387), ('get', 20343), ('good', 20260), ('fair', 20223), ('content', 20211), ('help', 20036), ('http', 19781), ('well', 19194), ('find', 19056), ('first', 18873), ('welcome', 18646), ('discussion', 18611), ('added', 18267), ('tag', 17600), ('links', 17595), ('look', 17531), ('read', 17494), ('say', 17407), ('1', 17071), ('Talk', 17041), ('believe', 16885), ('media', 16860), ('THE', 16833), ('take', 16775), ('images', 16748), ('list', 16644), ('work', 16637), ('much', 16497), ('go', 16338), ('point', 16301), ('removed', 16286), ('new', 16187), ('jpg', 15999), ('2', 15904), ('Your', 15798), ('many', 15765), ('There', 15459), ('non', 15355), ('two', 15276)]\n",
      "415617\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(100))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81266\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurance = 5\n",
    "vocab = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/train_' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_train.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/test' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_test.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.2\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "X = df_train.drop(target, axis=1)\n",
    "y = df_train[target]\n",
    "corpus = 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 62208\n",
      "Maximum length: 65\n",
      "CPU times: user 820 ms, sys: 37.2 ms, total: 857 ms\n",
      "Wall time: 859 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(vocab)\n",
    "\n",
    "#define vocab size and max len\n",
    "vocab_size = len(t.word_index) + 1\n",
    "max_length = 65\n",
    "\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.07 s, sys: 93.2 ms, total: 5.16 s\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# integer encode the documents\n",
    "encoded_Xtrain = t.texts_to_sequences(Xtrain[corpus].astype(str))\n",
    "encoded_Xtest = t.texts_to_sequences(Xtest[corpus].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents\n",
    "\n",
    "padded_train = pad_sequences(encoded_Xtrain, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_Xtest, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "CPU times: user 11 s, sys: 413 ms, total: 11.4 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/joaeechew/dev/glove.6B/glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', vocab_size=vocab_size, max_length=max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(6, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_path + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the model\n",
    "param_grid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('clf', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 65, 100)           4394900   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 58, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 29, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 928)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                9290      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 4,429,888\n",
      "Trainable params: 4,429,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "51120/51120 [==============================] - 161s 3ms/step - loss: 0.1047 - acc: 0.9565\n",
      "Epoch 2/2\n",
      "51120/51120 [==============================] - 160s 3ms/step - loss: 0.0448 - acc: 0.9830\n",
      "25560/25560 [==============================] - 6s 246us/step\n",
      "51120/51120 [==============================] - 13s 246us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 65, 100)           4394900   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 58, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 29, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 928)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                9290      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 4,429,888\n",
      "Trainable params: 4,429,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "51120/51120 [==============================] - 163s 3ms/step - loss: 0.0987 - acc: 0.9655\n",
      "Epoch 2/2\n",
      "51120/51120 [==============================] - 158s 3ms/step - loss: 0.0532 - acc: 0.9802\n",
      "25560/25560 [==============================] - 7s 259us/step\n",
      "51120/51120 [==============================] - 14s 269us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 65, 100)           4394900   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 58, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 29, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 928)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                9290      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 4,429,888\n",
      "Trainable params: 4,429,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "51120/51120 [==============================] - 160s 3ms/step - loss: 0.1144 - acc: 0.9534\n",
      "Epoch 2/2\n",
      "51120/51120 [==============================] - 169s 3ms/step - loss: 0.0558 - acc: 0.9786\n",
      "25560/25560 [==============================] - 7s 270us/step\n",
      "51120/51120 [==============================] - 13s 259us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 65, 100)           4394900   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 58, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 29, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 928)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                9290      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 4,429,888\n",
      "Trainable params: 4,429,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 17.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "76680/76680 [==============================] - 258s 3ms/step - loss: 0.0939 - acc: 0.9681\n",
      "Epoch 2/2\n",
      "76680/76680 [==============================] - 217s 3ms/step - loss: 0.0502 - acc: 0.9812\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# train the model\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, verbose=1, cv=3)\n",
    "grid_result = grid.fit(padded_train, ytrain)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best score {} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# save the model\n",
    "trained_model = grid_result.best_estimator_.named_steps['clf'].model\n",
    "model_path = os.path.join(dir_path, 'models', model_name)\n",
    "save_model(trained_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978506 (0.001198) with: {}\n"
     ]
    }
   ],
   "source": [
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1,5] = 61203 is not in [0, 43949)\n\t [[Node: embedding_13/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_13/embeddings/read, embedding_13/Cast)]]\n\nCaused by op 'embedding_13/Gather', defined at:\n  File \"//anaconda/envs/toxic/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/envs/toxic/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-141-8c9e1082ea42>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', '# fit the model\\ntarget = [\\'toxic\\', \\'severe_toxic\\', \\'obscene\\', \\'threat\\', \\'insult\\', \\'identity_hate\\']\\n\\n# train the model\\ngrid = GridSearchCV(pipeline, param_grid=param_grid, verbose=1, cv=3)\\ngrid_result = grid.fit(padded_train, ytrain)\\n\\n# summarize results\\nprint(\"Best {} : {} using {}\".format(label, grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n\\n# save the model\\ntrained_model = grid_result.best_estimator_.named_steps[\\'clf\\'].model\\nmodel_path = os.path.join(dir_path, \\'models\\', model_name)\\nsave_model(trained_model, model_path)')\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1238, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 6, in <module>\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 739, in fit\n    self.best_estimator_.fit(X, y, **fit_params)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/sklearn/pipeline.py\", line 250, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\", line 203, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\", line 136, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"<ipython-input-123-573835b78977>\", line 4, in create_model\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1193, in gather\n    return tf.gather(reference, indices)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1207, in gather\n    validate_indices=validate_indices, name=name)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[1,5] = 61203 is not in [0, 43949)\n\t [[Node: embedding_13/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_13/embeddings/read, embedding_13/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1,5] = 61203 is not in [0, 43949)\n\t [[Node: embedding_13/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_13/embeddings/read, embedding_13/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1,5] = 61203 is not in [0, 43949)\n\t [[Node: embedding_13/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_13/embeddings/read, embedding_13/Cast)]]\n\nCaused by op 'embedding_13/Gather', defined at:\n  File \"//anaconda/envs/toxic/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/envs/toxic/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-141-8c9e1082ea42>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', '# fit the model\\ntarget = [\\'toxic\\', \\'severe_toxic\\', \\'obscene\\', \\'threat\\', \\'insult\\', \\'identity_hate\\']\\n\\n# train the model\\ngrid = GridSearchCV(pipeline, param_grid=param_grid, verbose=1, cv=3)\\ngrid_result = grid.fit(padded_train, ytrain)\\n\\n# summarize results\\nprint(\"Best {} : {} using {}\".format(label, grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n\\n# save the model\\ntrained_model = grid_result.best_estimator_.named_steps[\\'clf\\'].model\\nmodel_path = os.path.join(dir_path, \\'models\\', model_name)\\nsave_model(trained_model, model_path)')\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1238, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 6, in <module>\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 739, in fit\n    self.best_estimator_.fit(X, y, **fit_params)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/sklearn/pipeline.py\", line 250, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\", line 203, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\", line 136, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"<ipython-input-123-573835b78977>\", line 4, in create_model\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1193, in gather\n    return tf.gather(reference, indices)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1207, in gather\n    validate_indices=validate_indices, name=name)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/envs/toxic/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[1,5] = 61203 is not in [0, 43949)\n\t [[Node: embedding_13/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_13/embeddings/read, embedding_13/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate model on test dataset\n",
    "y_pred = trained_model.predict(padded_test, verbose=1)\n",
    "loss = log_loss(ytest, y_pred)\n",
    "\n",
    "print(\"Combined log loss is {} .\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 39s 172us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# integer encode and pad test df\n",
    "encoded_submission = t.texts_to_sequences(df_test[corpus].astype(str))\n",
    "padded_submission = pad_sequences(encoded_submission, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_pred_proba = trained_model.predict(padded_submission, verbose=1)\n",
    "submission = pd.DataFrame(y_pred_proba, index=df_test.index, columns=target)\n",
    "\n",
    "## Output submissions\n",
    "path = 'data/submissions/' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "submission.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(y_pred_proba, index=df_test.index, columns=target)\n",
    "\n",
    "## Output submissions\n",
    "path = 'data/submissions/' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "submission.to_csv(full_path, header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
