{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a LSTM single model to text various cleaning steps and impact on score.\n",
    "\n",
    "Controls:\n",
    "- maxlen: 65\n",
    "- glove.6B.840D\n",
    "- epochs: 2\n",
    "- max features 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 95851 rows, 7 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/train.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_train = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 226998 rows, 1 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/test.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_test = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with string \"unknown\"\n",
    "df_train.fillna('unknown',inplace=True)\n",
    "df_test.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "test_size = 0.2\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "corpus = 'comment_text'\n",
    "\n",
    "X = df_train[corpus]\n",
    "y = df_train[target]\n",
    "\n",
    "\n",
    "X, X_HOO, y, y_HOO = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_path + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, regex='\\S+', remove_digits=False, english_only=False, stop_words=None, lower=True, filters=None):\n",
    "        self.regex = regex\n",
    "        self.remove_digits = remove_digits\n",
    "        self.english_only = english_only\n",
    "        self.stop_words = stop_words\n",
    "        self.lower = lower\n",
    "        self.filters = filters\n",
    "        \n",
    "    def transform(self, X, *args):\n",
    "        tokenizer = RegexpTokenizer(self.regex)\n",
    "        result = []\n",
    "        for row in X:\n",
    "            tokens = tokenizer.tokenize(row)\n",
    "            if self.filters is not None:\n",
    "                tokens = [re.sub(self.filters, '', t) for t in tokens]\n",
    "            if self.lower:\n",
    "                tokens = [t.lower() for t in tokens]\n",
    "            if self.remove_digits:\n",
    "                tokens = [t for t in tokens if not t.isdigit()]\n",
    "            if self.english_only:\n",
    "                english_words = set(nltk.corpus.words.words())\n",
    "                tokens = [t for t in tokens if t in english_words]\n",
    "            if self.stop_words is not None:\n",
    "                tokens = [t for t in tokens if not t in self.stop_words]\n",
    "            tokens = ' '.join(tokens)\n",
    "            if tokens == '':\n",
    "            \ttokens = 'cleaned'\n",
    "            result.append(tokens)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class KerasProcesser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_words, maxlen):\n",
    "        self.num_words = num_words\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def transform(self, X, *args):\n",
    "        tokenizer = Tokenizer(num_words=self.num_words)\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        result = tokenizer.texts_to_sequences(X)\n",
    "        result = pad_sequences(result, maxlen=self.maxlen, padding='post')\n",
    "        return result, tokenizer, self.maxlen\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(6, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/home/ec2-user/glove.840B.300d.txt', mode='rt', encoding='utf-8')\n",
    "for line in f:\n",
    "\tvalues = line.split(' ')\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit 2 models for 1st layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680,) split into (38340,) and (38340,)\n"
     ]
    }
   ],
   "source": [
    "split = round(len(X)/2)\n",
    "X_A = X.iloc[:split]\n",
    "y_A = y.iloc[:split]\n",
    "X_B = X.iloc[split:]\n",
    "y_B = y.iloc[split:]\n",
    "print(\"{} split into {} and {}\".format(X.shape, X_A.shape, X_B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline([\n",
    "    ('keraser', KerasProcesser(num_words=20000, maxlen=65))#,\n",
    "])\n",
    "\n",
    "param_grid = {\"keraser__num_words\": [20000],\n",
    "              \"keraser__maxlen\": [65],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming training data and test data...\n",
      "Embedding Glove pre-trained weights...\n",
      "Fitting model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 65, 300)           26246100  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 65, 100)           140400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 26,391,856\n",
      "Trainable params: 26,391,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "38340/38340 [==============================] - 421s 11ms/step - loss: 0.0740 - acc: 0.9755\n",
      "Epoch 2/2\n",
      "38340/38340 [==============================] - 420s 11ms/step - loss: 0.0455 - acc: 0.9830\n",
      "38340/38340 [==============================] - 37s 976us/step\n",
      "toxic log loss is 0.11111881449990804 .\n",
      "severe_toxic log loss is 0.023894290124504943 .\n",
      "obscene log loss is 0.05849954115930603 .\n",
      "threat log loss is 0.012755586184272373 .\n",
      "insult log loss is 0.06739538044531776 .\n",
      "identity_hate log loss is 0.02416503350442149 .\n",
      "Combined log loss: 0.049638107652955114 .\n",
      "Transforming training data and test data...\n",
      "Embedding Glove pre-trained weights...\n",
      "Fitting model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 65, 300)           26187600  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 65, 100)           140400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 26,333,356\n",
      "Trainable params: 26,333,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "38340/38340 [==============================] - 415s 11ms/step - loss: 0.0702 - acc: 0.9772\n",
      "Epoch 2/2\n",
      "38340/38340 [==============================] - 412s 11ms/step - loss: 0.0442 - acc: 0.9835\n",
      "38340/38340 [==============================] - 37s 962us/step\n",
      "toxic log loss is 0.11867673759204181 .\n",
      "severe_toxic log loss is 0.02362768168142543 .\n",
      "obscene log loss is 0.05995881133052097 .\n",
      "threat log loss is 0.012384265365141358 .\n",
      "insult log loss is 0.07251946247741578 .\n",
      "identity_hate log loss is 0.023353250622143766 .\n",
      "Combined log loss: 0.05175336817811486 .\n",
      "CPU times: user 2h 8min 48s, sys: 40min 50s, total: 2h 49min 39s\n",
      "Wall time: 29min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs = [\n",
    "    [X_A, X_B, y_A, y_B],\n",
    "    [X_B, X_A, y_B, y_A]\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for df in dfs:\n",
    "    for g in ParameterGrid(param_grid):\n",
    "        \n",
    "        # Set train tests\n",
    "        Xtrain = df[0]\n",
    "        ytrain = df[2]\n",
    "        Xtest = df[1]\n",
    "        ytest = df[3]\n",
    "        \n",
    "        print('Transforming training data and test data...')\n",
    "        p.set_params(**g)\n",
    "        padded_train, t, max_length = p.transform(Xtrain)\n",
    "        vocab_size = len(t.word_index) + 1\n",
    "        encoded_test = t.texts_to_sequences(Xtest)\n",
    "        padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "        # create a weight matrix for words in training docs\n",
    "        print('Embedding Glove pre-trained weights...')\n",
    "        embedding_matrix = np.zeros((vocab_size, 300))\n",
    "        for word, i in t.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        # fit model\n",
    "        print('Fitting model...')\n",
    "        model = KerasClassifier(build_fn=create_model, epochs=2, verbose=1)\n",
    "        model.fit(padded_train, ytrain, verbose=1)\n",
    "\n",
    "        # create predictions\n",
    "        y_pred = model.model.predict(padded_test, verbose=1)\n",
    "        y_pred = pd.DataFrame(y_pred, index=Xtest.index, columns=target)\n",
    "        losses = []\n",
    "\n",
    "        for label in target:\n",
    "            loss = log_loss(ytest[label], y_pred[label])\n",
    "            losses.append(loss)\n",
    "            print(\"{} log loss is {} .\".format(label, loss))\n",
    "        print(\"Combined log loss: {} .\".format(np.mean(losses)))\n",
    "\n",
    "        predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic log loss is 0.11489777604597493 .\n",
      "severe_toxic log loss is 0.023760985902965184 .\n",
      "obscene log loss is 0.0592291762449135 .\n",
      "threat log loss is 0.012569925774706866 .\n",
      "insult log loss is 0.06995742146136677 .\n",
      "identity_hate log loss is 0.02375914206328263 .\n",
      "Combined log loss: 0.05122455304682492 .\n"
     ]
    }
   ],
   "source": [
    "# Create stacking layer training set - LSTM predictions with true labels\n",
    "stacking = pd.concat([predictions[0], predictions[1]])\n",
    "for col in stacking.columns:\n",
    "    stacking.rename(columns={col: col+'_LSTM'}, inplace=True)\n",
    "stacking = stacking.join(X)\n",
    "stacking = stacking.join(y)\n",
    "\n",
    "# Evaluate 1st layer\n",
    "for label in target:\n",
    "    loss = log_loss(stacking[label], stacking[label+'_LSTM'])\n",
    "    losses.append(loss)\n",
    "    print(\"{} log loss is {} .\".format(label, loss))\n",
    "print(\"Combined log loss: {} .\".format(np.mean(losses)))\n",
    "\n",
    "# Save file\n",
    "path = 'data/processed/stacking.csv'\n",
    "full_path = os.path.join(dir_path, path)\n",
    "ytest.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HOO to evaluate first layer combined log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submissions\n",
    "encoded_submission = t.texts_to_sequences(df_test[corpus])\n",
    "padded_submission = pad_sequences(encoded_submission, maxlen=max_length, padding='post')\n",
    "y_submission = model.model.predict(padded_submission, verbose=1)\n",
    "submission = pd.DataFrame(y_submission, index=df_test.index, columns=target)\n",
    "path = 'data/submissions/' + model_name + '.csv'\n",
    "full_path = os.path.join(dir_path, path)\n",
    "submission.to_csv(full_path, header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
