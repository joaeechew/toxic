{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a default CNN single model to text various cleaning steps and impact on score.\n",
    "\n",
    "Controls:\n",
    "- CNN single model\n",
    "- maxlen: 65\n",
    "- min occurance vocab: 5\n",
    "- glove.6B.100D\n",
    "- epochs: 2\n",
    "- cv: 3\n",
    "- max features 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'raw_LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 95851 rows, 7 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/train.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_train = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 226998 rows, 1 columns.\n"
     ]
    }
   ],
   "source": [
    "path = 'data/raw/test.csv'\n",
    "\n",
    "full_path = os.path.join(dir_path, path)\n",
    "df_test = pd.read_csv(full_path, header=0, index_col=0)\n",
    "print(\"Dataset has {} rows, {} columns.\".format(*df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/joaeechew/dev/nltk_data\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from os import listdir\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(corpus, vocab, regex=r'[\\w]+', digits=False, english_only=False, stop=False, lemmatize=False):\n",
    "    \"\"\"Takes a corpus in list format and applies basic preprocessing steps of word tokenization,\n",
    "     removing of english stop words, and lemmatization. Returns processed corpus and vocab.\"\"\"\n",
    "    processed_corpus = []\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(regex)\n",
    "    for row in corpus:\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        if digits:\n",
    "            tokens = [t for t in tokens if not t.isdigit()]\n",
    "        if english_only:\n",
    "            tokens = [t for t in tokens if t in english_words]\n",
    "        if stopwords:\n",
    "            tokens = [t for t in tokens if not t in english_stopwords]\n",
    "        if lemmatize:\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "        vocab.update(tokens)\n",
    "        tokens = ' '.join(tokens)\n",
    "        if tokens == '':\n",
    "            tokens = 'cleaned'\n",
    "        processed_corpus.append(tokens)\n",
    "    return processed_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with string \"unknown\"\n",
    "df_train.fillna('unknown',inplace=True)\n",
    "df_test.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'[\\w|!]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vocab = Counter()\n",
    "# df_train.comment_text, vocab = process_text(df_train.comment_text, vocab,\n",
    "#                                             digits=False, english_only=False, stop=False, lemmatize=False)\n",
    "# df_test.comment_text, vocab = process_text(df_test.comment_text, vocab,\n",
    "#                                           digits=False, english_only=False, stop=False, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab.most_common(100))\n",
    "# # print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # keep tokens with a min occurrence\n",
    "# min_occurance = 5\n",
    "# vocab = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/train_' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_train.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/processed/test' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "df_test.to_csv(full_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.2\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "X = df_train.drop(target, axis=1)\n",
    "y = df_train[target]\n",
    "corpus = 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 153189\n",
      "Maximum length: 65\n",
      "CPU times: user 7.86 s, sys: 0 ns, total: 7.86 s\n",
      "Wall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(num_words=20000)\n",
    "t.fit_on_texts(df_train[corpus])\n",
    "\n",
    "#define vocab size and max len\n",
    "vocab_size = len(t.word_index) + 1\n",
    "max_length = 65\n",
    "\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.38 s, sys: 0 ns, total: 6.38 s\n",
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# integer encode the documents\n",
    "encoded_Xtrain = t.texts_to_sequences(Xtrain[corpus].astype(str))\n",
    "encoded_Xtest = t.texts_to_sequences(Xtest[corpus].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents\n",
    "\n",
    "padded_train = pad_sequences(encoded_Xtrain, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_Xtest, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "CPU times: user 14.3 s, sys: 0 ns, total: 14.3 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/home/ec2-user/glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create model, required for KerasClassifier\n",
    "# def create_model(optimizer='adam', vocab_size=vocab_size, max_length=max_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "#     model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "# #     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.add(Dense(6, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "#     # compile network\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     # summarize defined model\n",
    "#     model.summary()\n",
    "# #     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', vocab_size=vocab_size, max_length=max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(6, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_path):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_path + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 65, 100)           15318900  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 65, 100)           60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 15,384,656\n",
      "Trainable params: 15,384,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 69012 samples, validate on 7668 samples\n",
      "Epoch 1/2\n",
      "69012/69012 [==============================] - 540s 8ms/step - loss: 0.0786 - acc: 0.9762 - val_loss: 0.0564 - val_acc: 0.9802\n",
      "Epoch 2/2\n",
      "65376/69012 [===========================>..] - ETA: 28s - loss: 0.0494 - acc: 0.9822"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# train the model\n",
    "model.fit(padded_train, ytrain, validation_split=0.1)\n",
    "trained_model = model.model\n",
    "\n",
    "# save the model\n",
    "model_path = os.path.join(dir_path, 'models', model_name)\n",
    "save_model(trained_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19171/19171 [==============================] - 12s 626us/step\n",
      "[0.052028207666735951, 0.98183019338692057]\n"
     ]
    }
   ],
   "source": [
    "print(trained_model.evaluate(padded_test, ytest, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19171/19171 [==============================] - 12s 632us/step\n",
      "toxic log loss is 0.1142052628730768 .\n",
      "severe_toxic log loss is 0.023754155955640736 .\n",
      "obscene log loss is 0.05827334177400192 .\n",
      "threat log loss is 0.014348717349511727 .\n",
      "insult log loss is 0.07234214832882768 .\n",
      "identity_hate log loss is 0.02924561981465328 .\n",
      "Combined log loss is 0.0520282076826187 .\n",
      "CPU times: user 1min 6s, sys: 8.32 s, total: 1min 14s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pretty sure this needs to be looped and calculated column wise!\n",
    "\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_pred = trained_model.predict(padded_test, verbose=1)\n",
    "hold_out_preds = pd.DataFrame(y_pred, index=ytest.index, columns=target)\n",
    "losses = []\n",
    "\n",
    "for label in target:\n",
    "    loss = log_loss(ytest[label], hold_out_preds[label])\n",
    "    losses.append(loss)\n",
    "    print(\"{} log loss is {} .\".format(label, loss))\n",
    "    \n",
    "print(\"Combined log loss is {} .\".format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 146s 643us/step\n",
      "CPU times: user 13min 30s, sys: 1min 43s, total: 15min 13s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# integer encode and pad test df\n",
    "encoded_submission = t.texts_to_sequences(df_test[corpus].astype(str))\n",
    "padded_submission = pad_sequences(encoded_submission, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict\n",
    "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_pred_proba = trained_model.predict(padded_submission, verbose=1)\n",
    "submission = pd.DataFrame(y_pred_proba, index=df_test.index, columns=target)\n",
    "\n",
    "## Output submissions\n",
    "path = 'data/submissions/' + model_name + '.csv'\n",
    "\n",
    "dir_path = os.path.realpath('..')\n",
    "full_path = os.path.join(dir_path, path)\n",
    "\n",
    "submission.to_csv(full_path, header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
